{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67e1881",
   "metadata": {},
   "source": [
    "In Machine Learning and Optimization, convergence algorithms are methods that iteratively update model parameters until they reach a point where further improvement is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52ec8d",
   "metadata": {},
   "source": [
    "Convergence means: the algorithm has reached (or is very close to) the minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a407530",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ What does ‚ÄúConvergence‚Äù mean exactly?\n",
    "\n",
    "An algorithm converges when:\n",
    "\n",
    "The cost (loss) stops decreasing meaningfully, or\n",
    "\n",
    "Parameter updates become very small, or\n",
    "\n",
    "A predefined stopping condition is met.\n",
    "\n",
    "Mathematically, if \n",
    "J(Œ∏)\n",
    "J(Œ∏) is the cost function, convergence happens when:\n",
    "\n",
    "\n",
    "‚à£J(t+1)‚àíJ(t)‚à£‚âà0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6fcc4",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Why are Convergence Algorithms important?\n",
    "\n",
    "Because training a model is essentially:\n",
    "\n",
    "Finding parameters that minimize the cost function\n",
    "\n",
    "without convergence:\n",
    "\n",
    "Training may never finish \n",
    "\n",
    "Model may oscillate or divarge \n",
    "\n",
    "Results may be unstable or wrong "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ce858",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Gradient Descent ‚Äì The Core Convergence Algorithm\n",
    "\n",
    "Most convergence algorithms in ML are variants of Gradient Descent.\n",
    "\n",
    "Basic Idea \n",
    "\n",
    "compute the  gradient (slope) of the cost fn \n",
    "\n",
    "Move opposite to the gradient \n",
    "Repeat untill convergence \n",
    "\n",
    "Update rule :\n",
    "\n",
    "\n",
    "Œ∏:=Œ∏‚àíŒ±‚àáJ(Œ∏)\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "Œ∏\n",
    "Œ∏ = parameters (m, c, weights)\n",
    "\n",
    "Œ±\n",
    "Œ± = learning rate\n",
    "\n",
    "‚àáJ\n",
    "‚àáJ = gradient of cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fcb7f",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ Types of Convergence Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1bbe5",
   "metadata": {},
   "source": [
    "A. Batch Gradient Descent\n",
    "\n",
    "Uses entire dataset to compute gradient\n",
    "\n",
    "Smooth and stable convergence\n",
    "\n",
    "Slow for large datasets\n",
    "\n",
    "Converges:Steadily but slowly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7386eaf",
   "metadata": {},
   "source": [
    "B. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "Uses one data point at a time\n",
    "\n",
    "Faster but noisy updates\n",
    "\n",
    "Cost function fluctuates\n",
    "\n",
    "Converges: around minimum, not exactly at it . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fab5a7",
   "metadata": {},
   "source": [
    "C. Mini-Batch Gradient Descent\n",
    "\n",
    "Uses small batches of data\n",
    "\n",
    "Balance between speed and stability\n",
    "\n",
    "Most commonly used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f098d",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Convergence Graph (Intuition)\n",
    "\n",
    "X-axis: iterations / epochs\n",
    "\n",
    "Y-axis: cost (loss)\n",
    "\n",
    "Smooth downward curve ‚Üí good convergence\n",
    "\n",
    "Zig-zag curve ‚Üí SGD behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875a88d",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ Role of Learning Rate (Œ±) in Convergence\n",
    "\n",
    "üîπ Too Small Learning Rate\n",
    "\n",
    "Very slow convergence üê¢\n",
    "\n",
    "Takes many iterations\n",
    "\n",
    "üîπ Too Large Learning Rate\n",
    "\n",
    "Overshoots minimum\n",
    "\n",
    "May diverge ‚ùå\n",
    "\n",
    "üîπ Proper Learning Rate\n",
    "\n",
    "Fast and stable convergence ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc771b",
   "metadata": {},
   "source": [
    "7Ô∏è‚É£ Convex vs Non-Convex Cost Functions\n",
    "\n",
    "Convex Function\n",
    "\n",
    "Only one global minimum\n",
    "\n",
    "Guaranteed convergence\n",
    "\n",
    "Example: Linear Regression (MSE)\n",
    "\n",
    "Non-Convex Function\n",
    "\n",
    "Many local minima\n",
    "\n",
    "Convergence depends on initialization\n",
    "\n",
    "Example: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575005e1",
   "metadata": {},
   "source": [
    "8Ô∏è‚É£ Stopping Criteria (When do we stop?)\n",
    "\n",
    "An algorithm is considered converged when any one is satisfied:\n",
    "\n",
    "Cost change < threshold\n",
    "\n",
    "Gradient ‚âà 0\n",
    "\n",
    "Maximum iterations reached\n",
    "\n",
    "Validation loss starts increasing (early stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0b7eb",
   "metadata": {},
   "source": [
    "9Ô∏è‚É£ Common Convergence Problems\n",
    "\n",
    "| Problem        | Reason                   |\n",
    "| -------------- | ------------------------ |\n",
    "| No convergence | High learning rate       |\n",
    "| Very slow      | Low learning rate        |\n",
    "| Oscillation    | Poor scaling of features |\n",
    "| Local minimum  | Non-convex loss          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51318a01",
   "metadata": {},
   "source": [
    "üîü Improving Convergence\n",
    "\n",
    "Feature scaling / normalization\n",
    "\n",
    "Adaptive optimizers (Adam, RMSProp)\n",
    "\n",
    "Proper weight initialization\n",
    "\n",
    "Learning rate schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd19e6",
   "metadata": {},
   "source": [
    "One-Line Summary\n",
    "\n",
    "Convergence algorithms iteratively adjust model parameters until the cost function reaches its minimum and stops changing significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac7bd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
